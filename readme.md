# NLP Project: Pretrained Transformer Models Comparison

## Overview
Welcome to my NLP project where I've explored the fascinating world of pretrained transformer models available on Hugging Face (https://huggingface.co/). In this project, I've had the opportunity to gain practical experience with pretrained models and datasets provided by Hugging Face.

Hugging Face offers a treasure trove of pretrained transformer models that can serve as a fantastic starting point for various NLP tasks. This project allowed me to choose and compare different pretrained classification models to better understand their capabilities and performance.

### Project Goal
The main objective of this project was to select several pretrained classification models available on Hugging Face and compare their performance on a specific NLP classification task. This comparison involved using a dataset provided by Hugging Face and evaluating the accuracy of these models on at least 100 examples from the test set of the dataset.

In this project, I have:

1. Chosen and compared at least three different pretrained classification models, all focused on the same classification task.
OR
2. Chosen two different classification tasks and compared two pretrained models for each task.

I had the flexibility to select the classification tasks, but I ensured that both models and relevant datasets were available for the chosen tasks. It was perfectly fine if the models were not explicitly trained on that dataset but were relevant to the classification task.

The primary goal of this project was to gain practical experience with pretrained models and datasets, which can be valuable in future NLP projects and research.

## Hugging Face and Python
Hugging Face provides a Python library called "transformers" that simplifies working with pretrained models. This library made it incredibly easy to load pretrained models and utilize them for various NLP tasks. The [transformers documentation](https://huggingface.co/transformers/v3.0.2/index.html) provided extensive information on how to work with these models.

For example, when I was using a BERT-based model, I referred to the BERT documentation for specific examples. Otherwise, the `AutoModel` class turned out to be a helpful choice.

The `...ForSequenceClassification` models were ideal for my task. [Here's an example with BERT.](https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertforsequenceclassification)

Before diving into the main project, I ensured to explore and practice loading pretrained models from the documentation to get familiar with the library.

## Wrapping Up
This project was a fantastic opportunity to apply my NLP skills and explore the world of pretrained transformer models. The hands-on experience with Hugging Face's offerings has equipped me with valuable knowledge and skills that I can leverage in my future NLP projects and research.

Please feel free to reach out if you have any questions or need further information about this project.

